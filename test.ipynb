{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd26a69",
   "metadata": {},
   "source": [
    "1.导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cec5d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from src import data_label as dt, create_xlsx as cx, gene_idx as bf, ao_files as f\n",
    "from metabci.brainda.algorithms.decomposition.tdca import TDCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67931aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDecode:\n",
    "    def __init__(self, day):\n",
    "        super(RawDecode, self).__init__()\n",
    "        self.model = TDCA(0,1)\n",
    "        self.day = day  # 0: raw, 1: day1, 2: day2\n",
    "        self.t = 3 # time window\n",
    "        self.n_fold = 4\n",
    "\n",
    "\n",
    "    def raw_classify(self, yf, n_split, raw_data, raw_label):\n",
    "        '''\n",
    "        只跑TDCA的识别函数\n",
    "        Parameters:\n",
    "        model: classification model\n",
    "        yf: reference signal\n",
    "        n_split: number of cross validation\n",
    "        训练测试集的大小根据file_version来定 默认标注file_version=0 为1时翻倍\n",
    "        '''\n",
    "        # Container for loss\n",
    "        raw_loss = []\n",
    "        \n",
    "        kf = KFold(n_splits=n_split, random_state=None)\n",
    "        randseed_p1, randseed_p2 = 1, 3\n",
    "        iter_seed = 0\n",
    "        predict = 0\n",
    "        # Iterate through KFold splits\n",
    "        for train_index, test_index in kf.split(raw_data, raw_label):\n",
    "            # Data divide and Label divide\n",
    "            iter_seed += 0.1\n",
    "            data_train, data_test = raw_data[train_index], raw_data[test_index]\n",
    "            label_train, label_test = raw_label[train_index], raw_label[test_index]\n",
    "\n",
    "            if predict == 1:  # Single slice, train with 20, test with 20\n",
    "                # Data divide and Label divide\n",
    "                all_len, len1 = len(data_train), 20\n",
    "                indices_p1 = bf.generate_indices(all_len, len1, randseed_p1+iter_seed)\n",
    "                data_train, label_train = data_train[indices_p1], label_train[indices_p1]\n",
    "\n",
    "            elif predict == 3: # train with 40, test with 20\n",
    "                all_len, indice_2 = len(data_train), 40\n",
    "                indices_p2 = bf.generate_indices(all_len, indice_2, randseed_p2+iter_seed)\n",
    "                data_train, label_train = data_train[indices_p2], label_train[indices_p2]\n",
    "            \n",
    "            # get accuracy\n",
    "            label_pred = self.model.fit(data_train, label_train, yf).predict(data_test)\n",
    "            raw_loss.append(dt.loss(label_test, label_pred))\n",
    "        losses = [raw_loss]\n",
    "        loss = [np.round(100*np.mean(i), 2) for i in losses]\n",
    "        return loss\n",
    "\n",
    "    def raw_analyze(self, sub, filename, cap='new', file_version=0, over=False):\n",
    "        \"\"\"适用于原始数据分析的函数\n",
    "        训练集和测试集来着同一个受试者相同范式的数据\n",
    "        \"\"\"\n",
    "\n",
    "        fs = 500\n",
    "        raw_fileload = dt.fileload if file_version == 0 else dt.fileload_ant\n",
    "        pth_event_para1, pth_data_para1, pth_event_para2, pth_data_para2, pth_event_para3, pth_data_para3 = raw_fileload(filename)\n",
    "        print(f'len of event: {len(pth_event_para1)}, {len(pth_event_para2)}, {len(pth_event_para3)}')\n",
    "        print(f'len of data: {len(pth_data_para1)}, {len(pth_data_para2)}, {len(pth_data_para3)}')\n",
    "\n",
    "        # 数据预处理\n",
    "        data_para1, label_para1 = dt.filter_data(pth_event_para1, pth_data_para1, fs, self.t, cap)\n",
    "        data_para2, label_para2 = dt.filter_data(pth_event_para2, pth_data_para2, fs, self.t, cap)\n",
    "        # 参考信号\n",
    "        freq = dt.get_freq(event=pth_event_para1[0])[0]\n",
    "        yf = dt.reference_s(freq, 500, self.t)\n",
    "        # 训练模型解码\n",
    "        methods = [\"TDCA\"]\n",
    "        score1 = self.raw_classify(yf, self.n_fold, data_para1, label_para1)\n",
    "        score2 = self.raw_classify(yf, self.n_fold, data_para2, label_para2)\n",
    "        result_para1 = {\n",
    "            \"sub\": [sub],\n",
    "            \"method\": methods,\n",
    "            \"para1\": score1\n",
    "        }\n",
    "        result_para2 = {\n",
    "            \"sub\": [sub],\n",
    "            \"method\": methods,\n",
    "            \"para2\": score2\n",
    "        }\n",
    "        cx.save_data_to_excel(result_para1, 'para1', overwrite=over)\n",
    "        cx.save_data_to_excel(result_para2, 'para2', overwrite=over)\n",
    "\n",
    "        if pth_event_para3 and pth_data_para3:\n",
    "            data_para3, label_para3 = dt.filter_data(pth_event_para3, pth_data_para3, fs, self.t, cap)\n",
    "            mean_score3 = self.raw_classify(yf, self.n_fold, data_para3, label_para3)\n",
    "            result_para3 = {\n",
    "                \"sub\": [sub],\n",
    "                \"method\": methods,\n",
    "                \"para3\": mean_score3\n",
    "            }\n",
    "            cx.save_data_to_excel(result_para3, 'para3', overwrite=over)\n",
    "    \n",
    "        print('_.'*30)\n",
    "\n",
    "    def main(self):\n",
    "        # 获取数据和信息\n",
    "        infos = f.load_files()\n",
    "        files, caps, subs, versions = infos['files'][self.day], infos['caps'][self.day], infos['subs'][self.day], infos['versions'][self.day]\n",
    "        no_need = []\n",
    "        idx = [i for i in range(len(files)) if i not in no_need]\n",
    "        files, caps, subs, versions = [files[i] for i in idx], [caps[i] for i in idx], [subs[i] for i in idx], [versions[i] for i in idx]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # 离线解码分析\n",
    "        for i, (file, cap, sub, v) in enumerate(zip(files, caps, subs, versions)):\n",
    "            print(sub)\n",
    "            if self.day==0:\n",
    "                over = True if sub=='S1' else False\n",
    "            else:\n",
    "                over = True if sub=='S10' else False\n",
    "            self.raw_analyze(sub, file, cap, v, over)\n",
    "                \n",
    "        end = time.time()\n",
    "        print(f'Time used: {end - start:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451db407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已覆盖写入到 para1_results.xlsx\n",
      "数据已覆盖写入到 para2_results.xlsx\n",
      "数据已覆盖写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S2\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S3\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S4\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S5\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S6\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S7\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S8\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S9\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S10\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S11\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S12\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S13\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S14\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S15\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S16\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S17\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S18\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S19\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "S20\n",
      "len of event: 4, 4, 4\n",
      "len of data: 4, 4, 4\n",
      "数据已追加写入到 para1_results.xlsx\n",
      "数据已追加写入到 para2_results.xlsx\n",
      "数据已追加写入到 para3_results.xlsx\n",
      "_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._.\n",
      "Time used: 110.43s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    decoder = RawDecode(day=0)  # 0: raw, 1: day1, 2: day2\n",
    "    decoder.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74701090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ao_bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
